{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "xrvpafvd629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Falling back to CPU.\n",
      "  Note: Detection will be slower on CPU.\n",
      "\n",
      "--- Select Model Weights File ---\n",
      "Selected weights: C:/Users/danny/OneDrive - University of Arkansas/Research/Projects/BubbleID/models/instance_segmentation_weights.pth\n",
      "\n",
      "--- Select Input Image ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No image selected. Please run this cell again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 49\u001b[0m\n\u001b[0;32m     38\u001b[0m image_path \u001b[38;5;241m=\u001b[39m filedialog\u001b[38;5;241m.\u001b[39maskopenfilename(\n\u001b[0;32m     39\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect Input Image\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m     filetypes\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     initialdir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_path:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo image selected. Please run this cell again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: No image selected. Please run this cell again."
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# This cell sets up device (CUDA/CPU) and asks for file paths\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "\n",
    "# --- Check for CUDA and set device ---\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"CUDA available. Using GPU for inference.\")\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA not available. Falling back to CPU.\")\n",
    "    print(\"  Note: Detection will be slower on CPU.\")\n",
    "\n",
    "# --- Get model weights path ---\n",
    "print(\"\\n--- Select Model Weights File ---\")\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes('-topmost', True)\n",
    "\n",
    "model_weights_path = filedialog.askopenfilename(\n",
    "    title=\"Select Model Weights (.pth file)\",\n",
    "    filetypes=[(\"PyTorch Model\", \"*.pth\"), (\"All Files\", \"*.*\")],\n",
    "    initialdir=os.getcwd()\n",
    ")\n",
    "\n",
    "if not model_weights_path:\n",
    "    raise ValueError(\"No model weights selected. Please run this cell again.\")\n",
    "\n",
    "print(f\"Selected weights: {model_weights_path}\")\n",
    "\n",
    "# --- Get image path ---\n",
    "print(\"\\n--- Select Input Image ---\")\n",
    "image_path = filedialog.askopenfilename(\n",
    "    title=\"Select Input Image\",\n",
    "    filetypes=[\n",
    "        (\"TIFF files\", \"*.tif *.tiff\"),\n",
    "        (\"Image files\", \"*.png *.jpg *.jpeg\"),\n",
    "        (\"All Files\", \"*.*\")\n",
    "    ],\n",
    "    initialdir=os.getcwd()\n",
    ")\n",
    "\n",
    "if not image_path:\n",
    "    raise ValueError(\"No image selected. Please run this cell again.\")\n",
    "\n",
    "print(f\"Selected image: {image_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Configuration complete\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Weights: {os.path.basename(model_weights_path)}\")\n",
    "print(f\"Image: {os.path.basename(image_path)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35ad63-3a39-49ec-ac17-1866a473db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- LOADING MODEL ----------------------\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2 import model_zoo\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from scipy.ndimage import distance_transform_edt, maximum_filter, label as ndi_label\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import watershed\n",
    "\n",
    "# Use device from configuration cell\n",
    "cfg = get_cfg()\n",
    "cfg.OUTPUT_DIR = \"./\"\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 1000\n",
    "cfg.SOLVER.STEPS = []\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "# Use model weights path from configuration cell\n",
    "cfg.MODEL.WEIGHTS = model_weights_path\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4\n",
    "\n",
    "# Set device from configuration cell\n",
    "cfg.MODEL.DEVICE = device\n",
    "\n",
    "print(f\"Loading model on {device}...\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "\n",
    "# --------------------LOAD IMAGE --------------------------\n",
    "# Use image_path from configuration cell\n",
    "print(f\"\\nLoading image: {image_path}\")\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "if image is None:\n",
    "    raise ValueError(f\"Could not load image: {image_path}\")\n",
    "\n",
    "print(f\"âœ“ Image loaded Shape: {image.shape}\")\n",
    "\n",
    "import tifffile\n",
    "\n",
    "# --- Define crop box (modify these if you want to crop) ---\n",
    "# Set to None to skip cropping\n",
    "x_min, y_min = None, None  # BOTTOM LEFT (x, y)\n",
    "x_max, y_max = None, None  # TOP RIGHT (x, y)\n",
    "\n",
    "if x_min is not None and y_min is not None and x_max is not None and y_max is not None:\n",
    "    print(f\"\\nCropping: x from {x_min} to {x_max}, y from {y_min} to {y_max}\")\n",
    "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "    print(f\"Cropped shape: {cropped_image.shape}\")\n",
    "    \n",
    "    # Save the cropped image\n",
    "    input_dir = os.path.dirname(image_path)\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    cropped_img_path = os.path.join(input_dir, f\"{base_name}_cropped.tif\")\n",
    "    cv2.imwrite(cropped_img_path, cropped_image)\n",
    "    print(f\"Saved cropped image: {cropped_img_path}\")\n",
    "    \n",
    "    image = cropped_image\n",
    "else:\n",
    "    print(\"\\nSkipping crop - using full image\")\n",
    "\n",
    "# ------------------ INITIAL CROP------------------------------------\n",
    "print(\"\\nRunning detection (Pass 1/8: 6x8 grid)...\")\n",
    "rows = 6\n",
    "cols = 8\n",
    "crop_h = image.shape[0] // rows\n",
    "crop_w = image.shape[1] // cols\n",
    "fullimg = np.empty((rows, cols), dtype=object)\n",
    "\n",
    "for k in range(rows):\n",
    "    for j in range(cols):\n",
    "        outputs = predictor(image[(k*crop_h):(k*crop_h)+crop_h, (j*crop_w):(j*crop_w)+crop_w])\n",
    "        masks = outputs['instances'].pred_masks.cpu()\n",
    "        bb = outputs['instances'].pred_boxes\n",
    "\n",
    "        for l in range(len(bb)-1, -1, -1):\n",
    "            if bb[l].tensor.cpu().tolist()[0][0] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][1] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][2] > crop_w-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][3] > crop_h-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            combined_mask = torch.zeros_like(masks[0], dtype=torch.bool)\n",
    "            for i in range(len(masks)):\n",
    "                if masks[i].sum().item() < (crop_h*crop_w)//3:\n",
    "                    combined_mask |= masks[i]\n",
    "        else:\n",
    "            combined_mask = torch.zeros((crop_h, crop_w), dtype=torch.bool)\n",
    "\n",
    "        fullimg[k, j] = combined_mask\n",
    "\n",
    "img_rows = [np.concatenate([fullimg[i, j] for j in range(cols)], axis=1) for i in range(rows)]\n",
    "full_image = np.concatenate(img_rows, axis=0)\n",
    "\n",
    "bottom_pad = np.zeros((image.shape[0]-(crop_h*rows), full_image.shape[1]), dtype=bool)\n",
    "full_image = np.concatenate([full_image.astype(bool), bottom_pad], axis=0)\n",
    "\n",
    "right_pad = np.zeros((full_image.shape[0], image.shape[1]-(crop_w*cols)), dtype=bool)\n",
    "full_image = np.concatenate([full_image, right_pad], axis=1)\n",
    "\n",
    "plt.imshow(full_image)\n",
    "\n",
    "# ------------ CROP SHIFT RIGHT ---------------------\n",
    "print(\"Running detection (Pass 2/8: 6x7 grid, shift right)...\")\n",
    "fullimg = np.empty((rows, cols-1), dtype=object)\n",
    "\n",
    "for k in range(rows):\n",
    "    for j in range(cols-1):\n",
    "        outputs = predictor(image[(k*crop_h):(k*crop_h)+crop_h, (j*crop_w)+(crop_w//2):(j*crop_w)+crop_w+(crop_w//2)])\n",
    "        masks = outputs['instances'].pred_masks.cpu()\n",
    "        bb = outputs['instances'].pred_boxes\n",
    "\n",
    "        for l in range(len(bb)-1, -1, -1):\n",
    "            if bb[l].tensor.cpu().tolist()[0][0] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][1] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][2] > crop_w-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][3] > crop_h-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            combined_mask = torch.zeros_like(masks[0], dtype=torch.bool)\n",
    "            for i in range(len(masks)):\n",
    "                if masks[i].sum().item() < (crop_h*crop_w)//3:\n",
    "                    combined_mask |= masks[i]\n",
    "        else:\n",
    "            combined_mask = torch.zeros((crop_h, crop_w), dtype=torch.bool)\n",
    "\n",
    "        fullimg[k, j] = combined_mask\n",
    "\n",
    "img_rows = [np.concatenate([fullimg[i, j] for j in range(cols-1)], axis=1) for i in range(rows)]\n",
    "full_image1 = np.concatenate(img_rows, axis=0)\n",
    "\n",
    "left_pad = np.zeros((full_image1.shape[0], crop_w // 2), dtype=bool)\n",
    "add = 0\n",
    "if crop_w % 2 == 1:\n",
    "    add = 1\n",
    "right_pad = np.zeros((full_image1.shape[0], (crop_w // 2) + image.shape[1] - (crop_w*cols) + add), dtype=bool)\n",
    "full_image1 = np.concatenate([left_pad, full_image1, right_pad], axis=1)\n",
    "\n",
    "bottom_pad = np.zeros((image.shape[0]-(crop_h*rows), full_image1.shape[1]), dtype=bool)\n",
    "full_image1 = np.concatenate([full_image1.astype(bool), bottom_pad], axis=0)\n",
    "\n",
    "plt.imshow(full_image1)\n",
    "\n",
    "# ---------------- CROP SHIFT DOWN -------------------------\n",
    "print(\"Running detection (Pass 3/8: 5x8 grid, shift down)...\")\n",
    "fullimg = np.empty((rows-1, cols), dtype=object)\n",
    "\n",
    "for k in range(rows-1):\n",
    "    for j in range(cols):\n",
    "        outputs = predictor(\n",
    "            image[\n",
    "                (k*crop_h)+(crop_h//2):(k*crop_h)+crop_h+(crop_h//2),\n",
    "                (j*crop_w):(j*crop_w)+crop_w\n",
    "            ]\n",
    "        )\n",
    "        masks = outputs['instances'].pred_masks.cpu()\n",
    "        bb = outputs['instances'].pred_boxes\n",
    "\n",
    "        for l in range(len(bb)-1, -1, -1):\n",
    "            if bb[l].tensor.cpu().tolist()[0][0] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][1] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][2] > crop_w-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][3] > crop_h-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            combined_mask = torch.zeros_like(masks[0], dtype=torch.bool)\n",
    "            for i in range(len(masks)):\n",
    "                if masks[i].sum().item() < (crop_h*crop_w)//3:\n",
    "                    combined_mask |= masks[i]\n",
    "        else:\n",
    "            combined_mask = torch.zeros((crop_h, crop_w), dtype=torch.bool)\n",
    "\n",
    "        fullimg[k, j] = combined_mask\n",
    "\n",
    "img_rows = [np.concatenate([fullimg[i, j] for j in range(cols)], axis=1) for i in range(rows-1)]\n",
    "full_image2 = np.concatenate(img_rows, axis=0)\n",
    "add = 0\n",
    "if crop_h % 2 == 1:\n",
    "    add = 1\n",
    "top_pad = np.zeros((crop_h // 2, full_image2.shape[1]), dtype=bool)\n",
    "bottom_pad = np.zeros((crop_h//2 + image.shape[0] - (crop_h*rows) + add, full_image2.shape[1]), dtype=bool)\n",
    "full_image2 = np.concatenate([top_pad, full_image2.astype(bool), bottom_pad], axis=0)\n",
    "\n",
    "right_pad = np.zeros((full_image2.shape[0], image.shape[1]-(crop_w*cols)), dtype=bool)\n",
    "full_image2 = np.concatenate([full_image2, right_pad], axis=1)\n",
    "\n",
    "plt.imshow(full_image2)\n",
    "\n",
    "# --------------------- CROP SHIFT DOWN AND RIGHT --------------------\n",
    "print(\"Running detection (Pass 4/8: 5x7 grid, shift both)...\")\n",
    "fullimg = np.empty((rows-1, cols-1), dtype=object)\n",
    "\n",
    "for k in range(rows-1):\n",
    "    for j in range(cols-1):\n",
    "        outputs = predictor(\n",
    "            image[\n",
    "                (k*crop_h)+(crop_h//2):(k*crop_h)+crop_h+(crop_h//2),\n",
    "                (j*crop_w)+(crop_w//2):(j*crop_w)+crop_w+(crop_w//2)\n",
    "            ]\n",
    "        )\n",
    "        masks = outputs['instances'].pred_masks.cpu()\n",
    "        bb = outputs['instances'].pred_boxes\n",
    "\n",
    "        for l in range(len(bb)-1, -1, -1):\n",
    "            if bb[l].tensor.cpu().tolist()[0][0] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][1] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][2] > crop_w-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][3] > crop_h-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            combined_mask = torch.zeros_like(masks[0], dtype=torch.bool)\n",
    "            for i in range(len(masks)):\n",
    "                if masks[i].sum().item() < (crop_h*crop_w)//3:\n",
    "                    combined_mask |= masks[i]\n",
    "        else:\n",
    "            combined_mask = torch.zeros((crop_h, crop_w), dtype=torch.bool)\n",
    "\n",
    "        fullimg[k, j] = combined_mask\n",
    "\n",
    "img_rows = [np.concatenate([fullimg[i, j] for j in range(cols-1)], axis=1) for i in range(rows-1)]\n",
    "full_image3 = np.concatenate(img_rows, axis=0)\n",
    "add = 0\n",
    "if crop_h % 2 == 1:\n",
    "    add = 1\n",
    "top_pad = np.zeros((crop_h // 2, full_image3.shape[1]), dtype=bool)\n",
    "bottom_pad = np.zeros((crop_h//2 + image.shape[0] - (crop_h*rows) + add, full_image3.shape[1]), dtype=bool)\n",
    "full_image3 = np.concatenate([top_pad, full_image3.astype(bool), bottom_pad], axis=0)\n",
    "add = 0\n",
    "if crop_w % 2 == 1:\n",
    "    add = 1\n",
    "left_pad = np.zeros((full_image3.shape[0], crop_w // 2), dtype=bool)\n",
    "right_pad = np.zeros((full_image3.shape[0], (crop_w // 2) + image.shape[1] - (crop_w*cols) + add), dtype=bool)\n",
    "full_image3 = np.concatenate([left_pad, full_image3, right_pad], axis=1)\n",
    "\n",
    "plt.imshow(full_image3)\n",
    "\n",
    "full_mask = full_image | full_image1 | full_image2 | full_image3\n",
    "plt.imshow(full_mask)\n",
    "\n",
    "# ------------------- CROP LARGER ----------------------\n",
    "print(\"Running detection (Pass 5/8: 3x4 grid)...\")\n",
    "rows = 3\n",
    "cols = 4\n",
    "crop_h = image.shape[0] // rows\n",
    "crop_w = image.shape[1] // cols\n",
    "fullimg = np.empty((rows, cols), dtype=object)\n",
    "\n",
    "for k in range(rows):\n",
    "    for j in range(cols):\n",
    "        outputs = predictor(\n",
    "            image[\n",
    "                (k*crop_h):(k*crop_h)+crop_h,\n",
    "                (j*crop_w):(j*crop_w)+crop_w\n",
    "            ]\n",
    "        )\n",
    "        masks = outputs['instances'].pred_masks.cpu()\n",
    "        bb = outputs['instances'].pred_boxes\n",
    "\n",
    "        for l in range(len(bb)-1, -1, -1):\n",
    "            if bb[l].tensor.cpu().tolist()[0][0] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][1] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][2] > crop_w-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][3] > crop_h-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            combined_mask = torch.zeros_like(masks[0], dtype=torch.bool)\n",
    "            for i in range(len(masks)):\n",
    "                if masks[i].sum().item() < (crop_h*crop_w)//3:\n",
    "                    combined_mask |= masks[i]\n",
    "        else:\n",
    "            combined_mask = torch.zeros((crop_h, crop_w), dtype=torch.bool)\n",
    "\n",
    "        fullimg[k, j] = combined_mask\n",
    "\n",
    "img_rows = [np.concatenate([fullimg[i, j] for j in range(cols)], axis=1) for i in range(rows)]\n",
    "full_image4 = np.concatenate(img_rows, axis=0)\n",
    "\n",
    "bottom_pad = np.zeros((image.shape[0]-(crop_h*rows), full_image4.shape[1]), dtype=bool)\n",
    "full_image4 = np.concatenate([full_image4.astype(bool), bottom_pad], axis=0)\n",
    "\n",
    "right_pad = np.zeros((full_image4.shape[0], image.shape[1]-(crop_w*cols)), dtype=bool)\n",
    "full_image4 = np.concatenate([full_image4, right_pad], axis=1)\n",
    "\n",
    "plt.imshow(full_image4)\n",
    "\n",
    "# --------------------- CROP LARGE SHIFT RIGHT --------------------------\n",
    "print(\"Running detection (Pass 6/8: 3x3 grid, shift right)...\")\n",
    "fullimg = np.empty((rows, cols-1), dtype=object)\n",
    "\n",
    "for k in range(rows):\n",
    "    for j in range(cols-1):\n",
    "        outputs = predictor(\n",
    "            image[\n",
    "                (k*crop_h):(k*crop_h)+crop_h,\n",
    "                (j*crop_w)+(crop_w//2):(j*crop_w)+crop_w+(crop_w//2)\n",
    "            ]\n",
    "        )\n",
    "        masks = outputs['instances'].pred_masks.cpu()\n",
    "        bb = outputs['instances'].pred_boxes\n",
    "\n",
    "        for l in range(len(bb)-1, -1, -1):\n",
    "            if bb[l].tensor.cpu().tolist()[0][0] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][1] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][2] > crop_w-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][3] > crop_h-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            combined_mask = torch.zeros_like(masks[0], dtype=torch.bool)\n",
    "            for i in range(len(masks)):\n",
    "                if masks[i].sum().item() < (crop_h*crop_w)//3:\n",
    "                    combined_mask |= masks[i]\n",
    "        else:\n",
    "            combined_mask = torch.zeros((crop_h, crop_w), dtype=torch.bool)\n",
    "\n",
    "        fullimg[k, j] = combined_mask\n",
    "\n",
    "img_rows = [np.concatenate([fullimg[i, j] for j in range(cols-1)], axis=1) for i in range(rows)]\n",
    "full_image5 = np.concatenate(img_rows, axis=0)\n",
    "\n",
    "left_pad = np.zeros((full_image5.shape[0], crop_w // 2), dtype=bool)\n",
    "add = 0\n",
    "if crop_w % 2 == 1:\n",
    "    add = 1\n",
    "right_pad = np.zeros((full_image5.shape[0], (crop_w // 2) + image.shape[1] - (crop_w*cols) + add), dtype=bool)\n",
    "full_image5 = np.concatenate([left_pad, full_image5, right_pad], axis=1)\n",
    "\n",
    "bottom_pad = np.zeros((image.shape[0]-(crop_h*rows), full_image5.shape[1]), dtype=bool)\n",
    "full_image5 = np.concatenate([full_image5.astype(bool), bottom_pad], axis=0)\n",
    "\n",
    "plt.imshow(full_image5)\n",
    "\n",
    "# ------------------------ CROP LARGE SHIFT DOWN ---------------------\n",
    "print(\"Running detection (Pass 7/8: 2x4 grid, shift down)...\")\n",
    "fullimg = np.empty((rows-1, cols), dtype=object)\n",
    "\n",
    "for k in range(rows-1):\n",
    "    for j in range(cols):\n",
    "        outputs = predictor(\n",
    "            image[\n",
    "                (k*crop_h)+(crop_h//2):(k*crop_h)+crop_h+(crop_h//2),\n",
    "                (j*crop_w):(j*crop_w)+crop_w\n",
    "            ]\n",
    "        )\n",
    "        masks = outputs['instances'].pred_masks.cpu()\n",
    "        bb = outputs['instances'].pred_boxes\n",
    "\n",
    "        for l in range(len(bb)-1, -1, -1):\n",
    "            if bb[l].tensor.cpu().tolist()[0][0] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][1] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][2] > crop_w-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][3] > crop_h-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            combined_mask = torch.zeros_like(masks[0], dtype=torch.bool)\n",
    "            for i in range(len(masks)):\n",
    "                if masks[i].sum().item() < (crop_h*crop_w)//3:\n",
    "                    combined_mask |= masks[i]\n",
    "        else:\n",
    "            combined_mask = torch.zeros((crop_h, crop_w), dtype=torch.bool)\n",
    "\n",
    "        fullimg[k, j] = combined_mask\n",
    "\n",
    "img_rows = [np.concatenate([fullimg[i, j] for j in range(cols)], axis=1) for i in range(rows-1)]\n",
    "full_image6 = np.concatenate(img_rows, axis=0)\n",
    "\n",
    "add = 0\n",
    "if crop_h % 2 == 1:\n",
    "    add = 1\n",
    "top_pad = np.zeros((crop_h // 2, full_image6.shape[1]), dtype=bool)\n",
    "bottom_pad = np.zeros((crop_h//2 + image.shape[0] - (crop_h*rows) + add, full_image6.shape[1]), dtype=bool)\n",
    "full_image6 = np.concatenate([top_pad, full_image6.astype(bool), bottom_pad], axis=0)\n",
    "\n",
    "right_pad = np.zeros((full_image6.shape[0], image.shape[1]-(crop_w*cols)), dtype=bool)\n",
    "full_image6 = np.concatenate([full_image6, right_pad], axis=1)\n",
    "\n",
    "plt.imshow(full_image6)\n",
    "\n",
    "# --------------- CROP LARGE SHIFT DOWN AND RIGHT ----------------\n",
    "print(\"Running detection (Pass 8/8: 2x3 grid, shift both)...\")\n",
    "fullimg = np.empty((rows-1, cols-1), dtype=object)\n",
    "\n",
    "for k in range(rows-1):\n",
    "    for j in range(cols-1):\n",
    "        outputs = predictor(\n",
    "            image[\n",
    "                (k*crop_h)+(crop_h//2):(k*crop_h)+crop_h+(crop_h//2),\n",
    "                (j*crop_w)+(crop_w//2):(j*crop_w)+crop_w+(crop_w//2)\n",
    "            ]\n",
    "        )\n",
    "        masks = outputs['instances'].pred_masks.cpu()\n",
    "        bb=outputs['instances'].pred_boxes\n",
    "\n",
    "        for l in range(len(bb)-1, -1, -1):\n",
    "            if bb[l].tensor.cpu().tolist()[0][0] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][1] < 10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][2] > crop_w-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "            elif bb[l].tensor.cpu().tolist()[0][3] > crop_h-10:\n",
    "                masks = torch.cat((masks[:l], masks[l+1:]), dim=0)\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            combined_mask = torch.zeros_like(masks[0], dtype=torch.bool)\n",
    "            for i in range(len(masks)):\n",
    "                if masks[i].sum().item() < (crop_h*crop_w)//3:\n",
    "                    combined_mask |= masks[i]\n",
    "        else:\n",
    "            combined_mask = torch.zeros((crop_h, crop_w), dtype=torch.bool)\n",
    "\n",
    "        fullimg[k, j] = combined_mask\n",
    "\n",
    "img_rows = [np.concatenate([fullimg[i, j] for j in range(cols-1)], axis=1) for i in range(rows-1)]\n",
    "full_image7 = np.concatenate(img_rows, axis=0)\n",
    "\n",
    "add = 0\n",
    "if crop_h % 2 == 1:\n",
    "    add = 1\n",
    "top_pad = np.zeros((crop_h // 2, full_image7.shape[1]), dtype=bool)\n",
    "bottom_pad = np.zeros((crop_h//2 + image.shape[0] - (crop_h*rows) + add, full_image7.shape[1]), dtype=bool)\n",
    "full_image7 = np.concatenate([top_pad, full_image7.astype(bool), bottom_pad], axis=0)\n",
    "add = 0\n",
    "if crop_w % 2 == 1:\n",
    "    add = 1\n",
    "left_pad = np.zeros((full_image7.shape[0], crop_w // 2), dtype=bool)\n",
    "right_pad = np.zeros((full_image7.shape[0], (crop_w // 2) + image.shape[1] - (crop_w*cols) + add), dtype=bool)\n",
    "full_image7 = np.concatenate([left_pad, full_image7, right_pad], axis=1)\n",
    "\n",
    "plt.imshow(full_image7)\n",
    "\n",
    "# Combine all 8 passes\n",
    "print(\"\\nCombining all 8 detection passes...\")\n",
    "full_mask = (full_image | full_image1 | full_image2 | full_image3 | \n",
    "             full_image4 | full_image5 | full_image6 | full_image7)\n",
    "plt.imshow(full_mask)\n",
    "\n",
    "# ---- SAVE the full_mask with custom colormap ----\n",
    "mask_rgb = np.zeros((*full_mask.shape, 3), dtype=np.uint8)\n",
    "mask_rgb[full_mask == 0] = [128, 0, 128]  # Purple background\n",
    "mask_rgb[full_mask == 1] = [255, 255, 0]  # Yellow objects\n",
    "\n",
    "input_dir = os.path.dirname(image_path)\n",
    "base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "fullmask_path = os.path.join(input_dir, f\"{base_name}_fullmask.png\")\n",
    "\n",
    "cv2.imwrite(fullmask_path, cv2.cvtColor(mask_rgb, cv2.COLOR_RGB2BGR))\n",
    "print(f\"âœ“ Saved combined mask: {fullmask_path}\")\n",
    "\n",
    "# -------------- MARKER-BASED WATERSHED SPLITTING --------------\n",
    "print(\"\\nApplying watershed segmentation...\")\n",
    "mask_uint8 = (full_mask > 0).astype(np.uint8) * 255\n",
    "contour_data = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "clusters = contour_data[0] if len(contour_data) == 2 else contour_data[1]\n",
    "\n",
    "split_label_mask = np.zeros_like(mask_uint8, dtype=np.int32)\n",
    "current_label = 1\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_mask = np.zeros_like(mask_uint8)\n",
    "    cv2.drawContours(cluster_mask, [cluster], -1, 255, -1)\n",
    "    cluster_bool = cluster_mask > 0\n",
    "    dist = distance_transform_edt(cluster_bool)\n",
    "    local_max = (dist == maximum_filter(dist, size=15)) & (dist > 0)\n",
    "    markers_labeled, n_markers = ndi_label(local_max.astype(np.uint8))\n",
    "    if n_markers <= 1:\n",
    "        split_label_mask[cluster_bool] = current_label\n",
    "        current_label += 1\n",
    "        continue\n",
    "    labels = watershed(-dist, markers=markers_labeled, mask=cluster_bool)\n",
    "    for sublabel in range(1, labels.max() + 1):\n",
    "        split_label_mask[(labels == sublabel)] = current_label\n",
    "        current_label += 1\n",
    "\n",
    "# -------------- REGIONPROPS & SAVE RESULTS --------------\n",
    "print(\"Calculating region properties...\")\n",
    "regions = regionprops(split_label_mask)\n",
    "diameters = [region.equivalent_diameter for region in regions]\n",
    "\n",
    "print(f\"âœ“ Detected {len(regions)} droplets\")\n",
    "print(f\"  Mean diameter: {np.mean(diameters):.2f} pixels\")\n",
    "print(f\"  Std diameter: {np.std(diameters):.2f} pixels\")\n",
    "\n",
    "# Output file paths\n",
    "tif_path = os.path.join(input_dir, f\"{base_name}_mask.tif\")\n",
    "circles_img_path = os.path.join(input_dir, f\"{base_name}_circles.png\")\n",
    "diameter_txt_path = os.path.join(input_dir, f\"{base_name}_diameters.txt\")\n",
    "\n",
    "# --- CREATE & SAVE CIRCLE-FITTING/OVERLAY IMAGE ---\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(split_label_mask > 0, cmap='gray')\n",
    "for region in regions:\n",
    "    y, x = region.centroid\n",
    "    ediam = region.equivalent_diameter\n",
    "    circ = plt.Circle((x, y), ediam / 2, edgecolor='red', facecolor='none', linewidth=2)\n",
    "    ax.add_patch(circ)\n",
    "ax.set_axis_off()\n",
    "fig.tight_layout()\n",
    "plt.savefig(circles_img_path, bbox_inches='tight', pad_inches=0)\n",
    "plt.close(fig)\n",
    "\n",
    "# --- SAVE DIAMETERS AS TEXT FILE ---\n",
    "np.savetxt(diameter_txt_path, diameters)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"âœ“ Mask (PNG): {fullmask_path}\")\n",
    "print(f\"âœ“ Overlay with circles (PNG): {circles_img_path}\")\n",
    "print(f\"âœ“ Droplet diameters (TXT): {diameter_txt_path}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a09b00-6795-4206-9670-3cfadfc218ed",
   "metadata": {},
   "outputs": [],
   "source": "# Check if cropping was done - skip FOV processing if not\nif x_min is None or y_min is None:\n    print(\"Skipping FOV processing - no crop was applied to the image.\")\n    print(\"To enable FOV processing, set x_min, y_min, x_max, y_max values in the detection cell.\")\nelse:\n    import numpy as np\n    import cv2\n    import tifffile\n    import os\n    import matplotlib.pyplot as plt\n\n    # --- USER-SPECIFIED FOV from original image ---\n    fov_xmin_orig, fov_xmax_orig = 839, 955\n    fov_ymin_orig, fov_ymax_orig = 1460, 1579\n\n    # --- CROP ORIGIN from cell one ---\n    # x_min, y_min set above as 262, 614 -- do NOT change these here!\n\n    # --- Convert to cropped image coordinates ---\n    fov_xmin = fov_xmin_orig - x_min\n    fov_xmax = fov_xmax_orig - x_min\n    fov_ymin = fov_ymin_orig - y_min\n    fov_ymax = fov_ymax_orig - y_min\n\n    print(f\"Working with FOV on cropped image: x:{fov_xmin}:{fov_xmax}, y:{fov_ymin}:{fov_ymax}\")\n    print(f\"Image shape: {image.shape}\")\n\n    # --- Check bounds ---\n    H, W = image.shape[:2]\n    if not (0 <= fov_xmin < fov_xmax <= W) or not (0 <= fov_ymin < fov_ymax <= H):\n        raise ValueError(\"FOV crop indices are out of bounds! Please check input.\")\n\n    # --- Now crop your FOV in image and mask, then proceed as before ---\n    fov_img = image[fov_ymin:fov_ymax, fov_xmin:fov_xmax]\n    # etc: use fov_img, and the same slices for masks/overlays as in prior cell\n\n    # ========== 1. Crop the image in the FOV region ==========\n    fov_img = image[fov_ymin:fov_ymax, fov_xmin:fov_xmax]\n    fov_img_path = os.path.join(input_dir, f\"{base_name}_FOV_image.tif\")\n    tifffile.imwrite(fov_img_path, fov_img)  # Use tifffile to preserve depth/channels\n\n    # ========== 2. Find regions whose centroid is inside the FOV ==========\n    fov_regions = []\n    fov_indices = []\n    fov_mask = np.zeros_like(full_mask, dtype=bool)\n    for idx, region in enumerate(regions):\n        y, x = region.centroid\n        if fov_ymin <= y < fov_ymax and fov_xmin <= x < fov_xmax:\n            fov_regions.append(region)\n            fov_indices.append(idx)\n            fov_mask[full_mask == region.label] = True\n\n    # Crop the FOV mask to the FOV region\n    cropped_fov_mask = fov_mask[fov_ymin:fov_ymax, fov_xmin:fov_xmax]\n\n    # Color FOV mask: purple background, yellow objects\n    mask_rgb_fov = np.zeros((*cropped_fov_mask.shape, 3), dtype=np.uint8)\n    mask_rgb_fov[cropped_fov_mask == 0] = [128, 0, 128]\n    mask_rgb_fov[cropped_fov_mask == 1] = [255, 255, 0]\n    fov_mask_path = os.path.join(input_dir, f\"{base_name}_FOV_mask.png\")\n    cv2.imwrite(fov_mask_path, cv2.cvtColor(mask_rgb_fov, cv2.COLOR_RGB2BGR))\n\n    # ========== 3. FOV circles overlay ==========\n    fov_circles_path = os.path.join(input_dir, f\"{base_name}_FOV_circles.png\")\n    fig, ax = plt.subplots(figsize=(8,8))\n    ax.imshow(cropped_fov_mask, cmap=\"gray\", alpha=0.3)\n    ax.imshow(fov_img, alpha=0.8)\n    for region in fov_regions:\n        y, x = region.centroid\n        ediam = region.equivalent_diameter\n        # Draw only if centroid within cropped FOV\n        circ = plt.Circle((x-fov_xmin, y-fov_ymin), ediam/2, edgecolor='red', facecolor='none', linewidth=2)\n        ax.add_patch(circ)\n    ax.set_axis_off()\n    plt.tight_layout()\n    plt.savefig(fov_circles_path, bbox_inches='tight', pad_inches=0)\n    plt.close(fig)\n\n    # ========== 4. Save FOV diameters ==========\n    fov_diameters = np.array([region.equivalent_diameter for region in fov_regions])\n    fov_diam_path = os.path.join(input_dir, f\"{base_name}_FOV_diameters.txt\")\n    np.savetxt(fov_diam_path, fov_diameters)\n\n    print(f\"Saved FOV cropped image (tif): {fov_img_path}\")\n    print(f\"Saved FOV mask (png): {fov_mask_path}\")\n    print(f\"Saved FOV overlay with fitted circles (png): {fov_circles_path}\")\n    print(f\"Saved FOV droplet diameters (txt): {fov_diam_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e94ad4-acac-4c1c-9289-9e6130d4d6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubbleid-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}